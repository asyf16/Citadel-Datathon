{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Disclosure → Trading Lag Predictor\n",
    "\n",
    "**Question**: When a company discloses AI adoption, how many trading days until the stock sees peak abnormal returns?\n",
    "\n",
    "**Approach**: Event study + XGBoost regression model\n",
    "- Input: ticker, event details (use case, AI vendor, etc.)\n",
    "- Output: predicted lag in trading days to peak cumulative abnormal return (CAR)\n",
    "\n",
    "**Example**: Meta announces new AI release → model predicts peak returns in ~X trading days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Project imports\n",
    "sys.path.insert(0, '.')\n",
    "from src.mock_data import data_loader\n",
    "from src.event_study import run_event_study, get_average_car_curve, compute_returns\n",
    "from src.features import (\n",
    "    build_features, prepare_xy, train_model, evaluate_model,\n",
    "    mean_baseline_mae, predict_for_company,\n",
    "    CATEGORICAL_FEATURES, NUMERIC_FEATURES, TARGET\n",
    ")\n",
    "\n",
    "# Plot style\n",
    "sns.set_theme(style='whitegrid', font_scale=1.1)\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "SAVE_FIGURES = True\n",
    "FIG_DIR = 'outputs/figures'\n",
    "\n",
    "print('Setup complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (auto-detects real CSVs in data/ or falls back to mock)\n",
    "data = data_loader(seed=42)\n",
    "\n",
    "events = data['events']\n",
    "stock_prices = data['stock_prices']\n",
    "spy_prices = data['spy_prices']\n",
    "edgar_capex = data['edgar_capex']\n",
    "ticker_dim = data['ticker_dim']\n",
    "genai_releases = data['genai_releases']\n",
    "\n",
    "print('Data source:', 'REAL CSVs' if any(data['_using_real'].values()) else 'MOCK DATA')\n",
    "print(f'Events: {len(events):,} rows')\n",
    "print(f'Stock prices: {len(stock_prices):,} rows ({stock_prices[\"ticker\"].nunique()} tickers)')\n",
    "print(f'SPY prices: {len(spy_prices):,} rows')\n",
    "print(f'EDGAR fundamentals: {len(edgar_capex):,} rows')\n",
    "print(f'Ticker dimension: {len(ticker_dim):,} rows')\n",
    "print(f'GenAI releases: {len(genai_releases):,} rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick EDA: Event count by year\n",
    "events['year'] = pd.to_datetime(events['announcement_date']).dt.year\n",
    "year_counts = events['year'].value_counts().sort_index()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Events by year\n",
    "year_counts.plot(kind='bar', ax=axes[0], color='steelblue', edgecolor='black')\n",
    "axes[0].set_title('AI Disclosure Events by Year')\n",
    "axes[0].set_xlabel('Year')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Top sectors\n",
    "sector_counts = events.merge(ticker_dim[['ticker', 'sector']], on='ticker', how='left')['sector'].value_counts().head(8)\n",
    "sector_counts.plot(kind='barh', ax=axes[1], color='coral', edgecolor='black')\n",
    "axes[1].set_title('Events by Sector')\n",
    "axes[1].set_xlabel('Count')\n",
    "\n",
    "# Top AI vendors\n",
    "vendor_counts = events['ai_vendor'].dropna().value_counts().head(6)\n",
    "vendor_counts.plot(kind='barh', ax=axes[2], color='mediumpurple', edgecolor='black')\n",
    "axes[2].set_title('Events by AI Vendor')\n",
    "axes[2].set_xlabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "if SAVE_FIGURES:\n",
    "    plt.savefig(f'{FIG_DIR}/eda_overview.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "events.drop(columns=['year'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Event Study\n",
    "\n",
    "For each AI disclosure event:\n",
    "1. Compute daily stock returns and SPY returns around the event window (-30 to +60 days)\n",
    "2. Abnormal Return (AR) = stock return - SPY return\n",
    "3. Cumulative Abnormal Return (CAR) = running sum of daily ARs\n",
    "4. **Peak CAR day** = day with maximum abnormal return activity → this is our **target variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run event study over all events\n",
    "event_study_results = run_event_study(events, stock_prices, spy_prices)\n",
    "\n",
    "valid_results = event_study_results.dropna(subset=['peak_car_day'])\n",
    "print(f'Event study completed: {len(valid_results)}/{len(events)} events with valid results')\n",
    "print(f'\\nPeak CAR Day statistics:')\n",
    "print(valid_results['peak_car_day'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Average CAR curve with confidence band\n",
    "avg_car = get_average_car_curve(event_study_results, min_day=-5, max_day=60)\n",
    "\n",
    "if len(avg_car) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    ax.plot(avg_car['day'], avg_car['mean_car'], color='steelblue', linewidth=2, label='Mean CAR')\n",
    "    ax.fill_between(\n",
    "        avg_car['day'],\n",
    "        avg_car['mean_car'] - 1.96 * avg_car['std_car'] / np.sqrt(avg_car['count']),\n",
    "        avg_car['mean_car'] + 1.96 * avg_car['std_car'] / np.sqrt(avg_car['count']),\n",
    "        alpha=0.3, color='steelblue', label='95% CI'\n",
    "    )\n",
    "    ax.axvline(x=0, color='red', linestyle='--', alpha=0.7, label='Event Day')\n",
    "    ax.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "    ax.set_xlabel('Trading Days Relative to AI Disclosure')\n",
    "    ax.set_ylabel('Cumulative Abnormal Return (CAR)')\n",
    "    ax.set_title('Average Cumulative Abnormal Return Around AI Disclosure Events')\n",
    "    ax.legend()\n",
    "    \n",
    "    if SAVE_FIGURES:\n",
    "        plt.savefig(f'{FIG_DIR}/plot1_avg_car_curve.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No CAR curves available for plotting.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Distribution of peak_car_day\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "valid_peaks = valid_results['peak_car_day'].astype(int)\n",
    "ax.hist(valid_peaks, bins=range(1, 62), color='coral', edgecolor='black', alpha=0.8)\n",
    "ax.axvline(x=valid_peaks.mean(), color='navy', linestyle='--', linewidth=2,\n",
    "           label=f'Mean = {valid_peaks.mean():.1f} days')\n",
    "ax.axvline(x=valid_peaks.median(), color='darkgreen', linestyle='--', linewidth=2,\n",
    "           label=f'Median = {valid_peaks.median():.1f} days')\n",
    "ax.set_xlabel('Trading Days to Peak Abnormal Return')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Distribution of Peak CAR Day After AI Disclosure')\n",
    "ax.legend()\n",
    "\n",
    "if SAVE_FIGURES:\n",
    "    plt.savefig(f'{FIG_DIR}/plot2_peak_car_distribution.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical test: t-test on event-day abnormal returns\n",
    "ar_curves = event_study_results.attrs.get('ar_curves', [])\n",
    "day0_ars = [curve.get(0, np.nan) for curve in ar_curves]\n",
    "day0_ars = [x for x in day0_ars if not np.isnan(x)]\n",
    "\n",
    "if day0_ars:\n",
    "    t_stat, p_value = stats.ttest_1samp(day0_ars, 0)\n",
    "    print(f'Event-day abnormal return:')\n",
    "    print(f'  Mean AR at day 0: {np.mean(day0_ars):.4f} ({np.mean(day0_ars)*100:.2f}%)')\n",
    "    print(f'  t-statistic: {t_stat:.3f}')\n",
    "    print(f'  p-value: {p_value:.4f}')\n",
    "    print(f'  Significant at 5%: {\"Yes\" if p_value < 0.05 else \"No\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Feature Engineering\n",
    "\n",
    "Building ~15 features from multiple data sources:\n",
    "- **Event-level**: use_case, agent_type, has_ai_vendor, has_ai_model\n",
    "- **Company-level**: sector, industry, prior_event_count\n",
    "- **Market-level**: pre-event volatility, volume, 30-day return\n",
    "- **Fundamentals**: CapEx growth, R&D growth\n",
    "- **AI landscape**: days since last major GenAI release\n",
    "- **Time**: event_year, event_quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build feature matrix\n",
    "feature_matrix = build_features(\n",
    "    events, event_study_results, stock_prices,\n",
    "    edgar_capex, ticker_dim, genai_releases\n",
    ")\n",
    "\n",
    "print(f'Feature matrix: {feature_matrix.shape[0]} events × {feature_matrix.shape[1]} columns')\n",
    "print(f'\\nTarget variable (peak_car_day):')\n",
    "print(feature_matrix[TARGET].describe())\n",
    "\n",
    "# Show feature columns\n",
    "feature_cols = [c for c in CATEGORICAL_FEATURES + NUMERIC_FEATURES if c in feature_matrix.columns]\n",
    "print(f'\\nFeature columns ({len(feature_cols)}):')\n",
    "for i, col in enumerate(feature_cols, 1):\n",
    "    print(f'  {i}. {col}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value summary\n",
    "missing = feature_matrix[feature_cols].isnull().sum()\n",
    "missing_pct = (missing / len(feature_matrix) * 100).round(1)\n",
    "missing_df = pd.DataFrame({'Missing': missing, '% Missing': missing_pct})\n",
    "print('Missing values per feature:')\n",
    "print(missing_df[missing_df['Missing'] > 0].to_string() if missing_df['Missing'].sum() > 0 else '  None!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature correlation heatmap (numeric features only)\n",
    "numeric_cols = [c for c in NUMERIC_FEATURES if c in feature_matrix.columns]\n",
    "corr_cols = numeric_cols + [TARGET]\n",
    "corr_matrix = feature_matrix[corr_cols].corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='RdBu_r',\n",
    "            center=0, square=True, ax=ax, vmin=-1, vmax=1)\n",
    "ax.set_title('Feature Correlation Heatmap')\n",
    "\n",
    "if SAVE_FIGURES:\n",
    "    plt.savefig(f'{FIG_DIR}/feature_correlation.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Model Training & Evaluation\n",
    "\n",
    "- **Split**: Time-based (80% train / 20% test by announcement_date)\n",
    "- **Baselines**: Mean predictor, Ridge regression\n",
    "- **Model**: XGBRegressor\n",
    "- **Evaluation**: MAE, RMSE, R²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X, y, label_encoders = prepare_xy(feature_matrix)\n",
    "feature_col_names = list(X.columns)\n",
    "\n",
    "# Time-based split\n",
    "n = len(X)\n",
    "split_idx = int(n * 0.8)\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "print(f'Train set: {len(X_train)} samples')\n",
    "print(f'Test set:  {len(X_test)} samples')\n",
    "print(f'Train period: {feature_matrix[\"announcement_date\"].iloc[:split_idx].min().date()} to {feature_matrix[\"announcement_date\"].iloc[:split_idx].max().date()}')\n",
    "print(f'Test period:  {feature_matrix[\"announcement_date\"].iloc[split_idx:].min().date()} to {feature_matrix[\"announcement_date\"].iloc[split_idx:].max().date()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline 1: Mean predictor\n",
    "baseline_mae = mean_baseline_mae(y_test)\n",
    "print(f'Mean Baseline MAE: {baseline_mae:.2f} days')\n",
    "\n",
    "# Baseline 2: Ridge regression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "ridge = Ridge(alpha=10)\n",
    "ridge.fit(X_train.fillna(0), y_train)\n",
    "ridge_pred = np.clip(ridge.predict(X_test.fillna(0)), 1, 60)\n",
    "ridge_mae = mean_absolute_error(y_test, ridge_pred)\n",
    "ridge_rmse = np.sqrt(mean_squared_error(y_test, ridge_pred))\n",
    "ridge_r2 = r2_score(y_test, ridge_pred)\n",
    "print(f'Ridge MAE: {ridge_mae:.2f} days, RMSE: {ridge_rmse:.2f}, R²: {ridge_r2:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost model\n",
    "xgb_model = train_model(X_train, y_train)\n",
    "xgb_metrics = evaluate_model(xgb_model, X_test, y_test)\n",
    "\n",
    "print(f'XGBoost Results:')\n",
    "print(f'  MAE:  {xgb_metrics[\"MAE\"]:.2f} days')\n",
    "print(f'  RMSE: {xgb_metrics[\"RMSE\"]:.2f} days')\n",
    "print(f'  R²:   {xgb_metrics[\"R2\"]:.3f}')\n",
    "print(f'\\nImprovement over mean baseline: {(1 - xgb_metrics[\"MAE\"]/baseline_mae)*100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3: Model comparison bar chart\n",
    "model_names = ['Mean Baseline', 'Ridge Regression', 'XGBoost']\n",
    "maes = [baseline_mae, ridge_mae, xgb_metrics['MAE']]\n",
    "colors = ['#95a5a6', '#3498db', '#e74c3c']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "bars = ax.bar(model_names, maes, color=colors, edgecolor='black', width=0.6)\n",
    "\n",
    "for bar, mae in zip(bars, maes):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.2,\n",
    "            f'{mae:.2f}', ha='center', va='bottom', fontweight='bold', fontsize=12)\n",
    "\n",
    "ax.set_ylabel('Mean Absolute Error (trading days)')\n",
    "ax.set_title('Model Comparison: Predicting Days to Peak Abnormal Return')\n",
    "ax.set_ylim(0, max(maes) * 1.3)\n",
    "\n",
    "if SAVE_FIGURES:\n",
    "    plt.savefig(f'{FIG_DIR}/plot3_model_comparison.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Metrics table\n",
    "metrics_table = pd.DataFrame({\n",
    "    'Model': model_names,\n",
    "    'MAE (days)': [f'{m:.2f}' for m in maes],\n",
    "    'RMSE (days)': [f'{np.sqrt(mean_squared_error(y_test, np.full(len(y_test), y_test.mean()))):.2f}',\n",
    "                    f'{ridge_rmse:.2f}', f'{xgb_metrics[\"RMSE\"]:.2f}'],\n",
    "    'R²': [f'{0:.3f}', f'{ridge_r2:.3f}', f'{xgb_metrics[\"R2\"]:.3f}'],\n",
    "})\n",
    "print(metrics_table.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 4: Actual vs Predicted scatter\n",
    "y_pred = xgb_metrics['y_pred']\n",
    "y_actual = xgb_metrics['y_test']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.scatter(y_actual, y_pred, alpha=0.6, s=60, c='steelblue', edgecolor='white', linewidth=0.5)\n",
    "\n",
    "# Perfect prediction line\n",
    "lims = [0, 65]\n",
    "ax.plot(lims, lims, 'r--', alpha=0.7, label='Perfect prediction')\n",
    "\n",
    "# Trend line\n",
    "z = np.polyfit(y_actual, y_pred, 1)\n",
    "p = np.poly1d(z)\n",
    "x_line = np.linspace(min(y_actual), max(y_actual), 100)\n",
    "ax.plot(x_line, p(x_line), 'g-', alpha=0.7, linewidth=2, label=f'Trend (slope={z[0]:.2f})')\n",
    "\n",
    "ax.set_xlabel('Actual Peak CAR Day')\n",
    "ax.set_ylabel('Predicted Peak CAR Day')\n",
    "ax.set_title('XGBoost: Actual vs Predicted Days to Peak Return')\n",
    "ax.set_xlim(lims)\n",
    "ax.set_ylim(lims)\n",
    "ax.legend()\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "if SAVE_FIGURES:\n",
    "    plt.savefig(f'{FIG_DIR}/plot4_actual_vs_predicted.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 5: SHAP feature importance\n",
    "import shap\n",
    "\n",
    "explainer = shap.TreeExplainer(xgb_model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Bar plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "plt.sca(axes[0])\n",
    "shap.summary_plot(shap_values, X_test, plot_type='bar', show=False,\n",
    "                  max_display=15)\n",
    "axes[0].set_title('SHAP Feature Importance (Bar)')\n",
    "\n",
    "# Beeswarm plot\n",
    "plt.sca(axes[1])\n",
    "shap.summary_plot(shap_values, X_test, show=False, max_display=15)\n",
    "axes[1].set_title('SHAP Feature Impact (Beeswarm)')\n",
    "\n",
    "plt.tight_layout()\n",
    "if SAVE_FIGURES:\n",
    "    plt.savefig(f'{FIG_DIR}/plot5_shap_importance.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Demo — Predict for Meta\n",
    "\n",
    "Simulate Meta announcing a new AI initiative and predict how many trading days until peak abnormal returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict for Meta\n",
    "meta_event = {\n",
    "    'use_case': 'Content Creation',\n",
    "    'agent_type': 'Copilot',\n",
    "    'ai_vendor': 'Meta AI',\n",
    "    'ai_model': 'Llama',\n",
    "    'announcement_date': '2025-01-15',\n",
    "    'prior_event_count': 3,\n",
    "}\n",
    "\n",
    "meta_result = predict_for_company(\n",
    "    model=xgb_model,\n",
    "    ticker='META',\n",
    "    event_info=meta_event,\n",
    "    stock_prices=stock_prices,\n",
    "    edgar_df=edgar_capex,\n",
    "    ticker_dim=ticker_dim,\n",
    "    genai_dim=genai_releases,\n",
    "    label_encoders=label_encoders,\n",
    "    feature_cols=feature_col_names,\n",
    ")\n",
    "\n",
    "print('=' * 60)\n",
    "print(f'  PREDICTION: META AI Disclosure')\n",
    "print(f'  Event: {meta_event[\"use_case\"]} using {meta_event[\"ai_model\"]}')\n",
    "print(f'  Announcement: {meta_event[\"announcement_date\"]}')\n",
    "print(f'')\n",
    "print(f'  >>> Predicted peak returns in {meta_result[\"predicted_lag\"]} trading days <<<')\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 6: SHAP force plot for Meta prediction\n",
    "meta_shap = explainer.shap_values(meta_result['feature_vector'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 3))\n",
    "shap.force_plot(\n",
    "    explainer.expected_value,\n",
    "    meta_shap[0],\n",
    "    meta_result['feature_vector'].iloc[0],\n",
    "    matplotlib=True,\n",
    "    show=False\n",
    ")\n",
    "plt.title(f'SHAP Explanation: META Prediction ({meta_result[\"predicted_lag\"]} days)', fontsize=13, pad=60)\n",
    "\n",
    "if SAVE_FIGURES:\n",
    "    plt.savefig(f'{FIG_DIR}/plot6_shap_force_meta.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 7: Meta's historical AI events with CAR curves\n",
    "meta_events = events[events['ticker'] == 'META']\n",
    "meta_study = event_study_results[event_study_results['ticker'] == 'META']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "car_curves = event_study_results.attrs.get('car_curves', [])\n",
    "meta_indices = meta_study.index.tolist()\n",
    "\n",
    "# Get indices mapping from event_study_results to car_curves list\n",
    "valid_mask = event_study_results['peak_car_day'].notna()\n",
    "valid_indices = event_study_results[valid_mask].index.tolist()\n",
    "\n",
    "colors_meta = plt.cm.viridis(np.linspace(0.2, 0.8, len(meta_indices)))\n",
    "plotted = 0\n",
    "\n",
    "for i, meta_idx in enumerate(meta_indices):\n",
    "    if meta_idx in valid_indices:\n",
    "        curve_pos = valid_indices.index(meta_idx)\n",
    "        if curve_pos < len(car_curves):\n",
    "            curve = car_curves[curve_pos]\n",
    "            days = sorted(curve.keys())\n",
    "            vals = [curve[d] for d in days]\n",
    "            event_date = meta_study.loc[meta_idx, 'announcement_date']\n",
    "            peak_day = int(meta_study.loc[meta_idx, 'peak_car_day'])\n",
    "            ax.plot(days, vals, color=colors_meta[i % len(colors_meta)],\n",
    "                    linewidth=1.5, alpha=0.8,\n",
    "                    label=f'{str(event_date.date())} (peak: day {peak_day})')\n",
    "            plotted += 1\n",
    "\n",
    "if plotted > 0:\n",
    "    ax.axvline(x=0, color='red', linestyle='--', alpha=0.5, label='Event Day')\n",
    "    ax.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "    ax.set_xlabel('Trading Days Relative to Event')\n",
    "    ax.set_ylabel('Cumulative Abnormal Return (CAR)')\n",
    "    ax.set_title('META: CAR Curves for Historical AI Disclosure Events')\n",
    "    ax.legend(fontsize=9, loc='upper left')\n",
    "else:\n",
    "    ax.text(0.5, 0.5, 'No META events found in dataset',\n",
    "            ha='center', va='center', transform=ax.transAxes, fontsize=14)\n",
    "    ax.set_title('META: Historical AI Events')\n",
    "\n",
    "if SAVE_FIGURES:\n",
    "    plt.savefig(f'{FIG_DIR}/plot7_meta_car_curves.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Conclusions\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **AI disclosure events generate detectable abnormal returns** in the post-announcement window, with peak activity occurring at varying lags depending on event and company characteristics.\n",
    "\n",
    "2. **The XGBoost model outperforms simple baselines** (mean predictor and Ridge regression) at predicting the number of trading days to peak abnormal return after an AI disclosure.\n",
    "\n",
    "3. **Most important predictive features** (from SHAP analysis) include:\n",
    "   - Use case type (e.g., chatbot vs. predictive analytics)\n",
    "   - Company sector and industry\n",
    "   - Pre-event market conditions (volatility, volume)\n",
    "   - Timing relative to major GenAI model releases\n",
    "\n",
    "### Practical Implications\n",
    "\n",
    "- Investors can use the model to estimate *when* to expect peak market reaction after an AI disclosure\n",
    "- Different types of AI adoption (e.g., customer-facing chatbot vs. internal analytics) may have different market absorption timelines\n",
    "- Companies in technology sectors may see faster price reactions than those in traditional industries\n",
    "\n",
    "### Limitations & Future Work\n",
    "\n",
    "- **Current data**: Results shown here use mock data; real-world performance will differ\n",
    "- **Estimation window**: The market-adjusted model (stock - SPY) is a simplified approach; Fama-French multi-factor models could improve abnormal return estimation\n",
    "- **Additional features**: Sentiment analysis of disclosure text, market-wide volatility (VIX), and concurrent events could improve predictions\n",
    "- **Non-linear lags**: Some events may have multiple reaction waves; the current model predicts a single peak day"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
